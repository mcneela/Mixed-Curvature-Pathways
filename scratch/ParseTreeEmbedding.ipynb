{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import scipy.sparse.csgraph as csg\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "import networkx as nx\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import math\n",
    "\n",
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "import learning_util as lu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distortion calculations\n",
    "\n",
    "def entry_is_good(h, h_rec): return (not torch.isnan(h_rec)) and (not torch.isinf(h_rec)) and h_rec != 0 and h != 0\n",
    "\n",
    "def distortion_entry(h,h_rec,me,mc):\n",
    "    avg = abs(h_rec - h)/h\n",
    "    if h_rec/h > me: me = h_rec/h\n",
    "    if h/h_rec > mc: mc = h/h_rec\n",
    "    return (avg,me,mc)\n",
    "\n",
    "def distortion_row(H1, H2, n, row):\n",
    "    mc, me, avg, good = 0,0,0,0\n",
    "    for i in range(n):\n",
    "        if i != row and entry_is_good(H1[i], H2[i]):\n",
    "            (_avg,me,mc) = distortion_entry(H1[i], H2[i],me,mc)\n",
    "            good        += 1\n",
    "            avg         += _avg\n",
    "    avg /= good if good > 0 else 1.0\n",
    "    return (mc, me, avg, n-1-good)\n",
    "\n",
    "def distortion(H1, H2, n, jobs=16):\n",
    "#     dists = Parallel(n_jobs=jobs)(delayed(distortion_row)(H1[i,:],H2[i,:],n,i) for i in range(n))\n",
    "    dists = (distortion_row(H1[i,:],H2[i,:],n,i) for i in range(n))\n",
    "    avg = torch.stack([tup[2] for tup in dists]).sum()/n\n",
    "    return avg\n",
    "\n",
    "\n",
    "#Loading the graph and getting the distance matrix.\n",
    "\n",
    "def load_graph(file_name, directed=False):\n",
    "    G = nx.DiGraph() if directed else nx.Graph()\n",
    "    with open(file_name, \"r\") as f:\n",
    "        for line in f:\n",
    "            tokens = line.split()\n",
    "            u = int(tokens[0])\n",
    "            v = int(tokens[1])\n",
    "            if len(tokens) > 2:\n",
    "                w = float(tokens[2])\n",
    "                G.add_edge(u, v, weight=w)\n",
    "            else:\n",
    "                G.add_edge(u,v)\n",
    "    return G\n",
    "\n",
    "\n",
    "def compute_row(i, adj_mat): \n",
    "    return csg.dijkstra(adj_mat, indices=[i], unweighted=True, directed=False)\n",
    "\n",
    "def get_dist_mat(G):\n",
    "    n = G.order()\n",
    "    adj_mat = nx.to_scipy_sparse_matrix(G, nodelist=list(range(G.order())))\n",
    "    t = time.time()\n",
    "    \n",
    "    num_cores = multiprocessing.cpu_count()\n",
    "    dist_mat = Parallel(n_jobs=num_cores)(delayed(compute_row)(i,adj_mat) for i in range(n))\n",
    "    dist_mat = np.vstack(dist_mat)\n",
    "    return dist_mat\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n",
    "\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Riemannian SGD\n",
    "\n",
    "import glob\n",
    "from torch.optim import Optimizer\n",
    "\n",
    "class RiemannianSGD(Optimizer):\n",
    "    \"\"\"Riemannian stochastic gradient descent.\n",
    "    Args:\n",
    "        params (iterable): iterable of parameters to optimize or dicts defining\n",
    "            parameter groups\n",
    "        lr (float): learning rate\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, params, lr):\n",
    "        # if lr is not required and lr < 0.0:\n",
    "        #     raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
    "        defaults = dict(lr=lr)\n",
    "        super(RiemannianSGD, self).__init__(params, defaults)\n",
    "\n",
    "    def step(self):\n",
    "        \"\"\"Performs a single optimization step.\n",
    "        Arguments:\n",
    "            lr (float): learning rate for the current update.\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                d_p = p.grad.data\n",
    "                lr = group['lr']\n",
    "\n",
    "            # try:\n",
    "            #     hyper = p.is_hyper\n",
    "            # except:\n",
    "            #     hyper = False\n",
    "\n",
    "            # if hyper:\n",
    "            p.data.add_(hyperbolic_step(p.data, d_p, lr))\n",
    "            # else:\n",
    "            #     p.data.add_(-lr, d_p)\n",
    "\n",
    "        return loss\n",
    "\n",
    "def batch_dot(u, v):\n",
    "    return torch.sum(u * v, dim=-1, keepdim=True)\n",
    "\n",
    "def natural_grad(v, dv):\n",
    "    vnorm_squared = batch_dot(v, v)\n",
    "    dv = dv * ((1 - vnorm_squared) ** 2 / 4).expand_as(dv)\n",
    "    return dv\n",
    "\n",
    "def batch_add(u, v, c=1):\n",
    "    numer = 1 + 2 * batch_dot(u, v) + batch_dot(v, v) * u + (1 - batch_dot(u, u)) * v\n",
    "    denom = 1 + 2 * batch_dot(u, v) + batch_dot(v, v) * batch_dot(u, u)\n",
    "\n",
    "    return numer/denom\n",
    "\n",
    "def batch_exp_map(x, v, c=1):\n",
    "    term = torch.tanh((torch.norm(v, dim=-1, keepdim=True) / (1 - torch.norm(x, dim=-1, keepdim=True).pow(2)))) * \\\n",
    "                 (v/(torch.norm(v, dim=-1, keepdim=True)))\n",
    "    return batch_add(x, term, c)\n",
    "\n",
    "def hyperbolic_step(param, grad, lr):\n",
    "    ngrad = natural_grad(param, grad)\n",
    "    return batch_exp_map(param, -lr * ngrad, c=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {}\n",
    "        self.n_words = 0\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for token in sentence:\n",
    "            self.addWord(token['form'])\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "            \n",
    "\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from conllu import parse_tree, parse_tree_incr, parse, parse_incr\n",
    "from io import open\n",
    "import scipy.sparse.csgraph as csg\n",
    "import networkx as nx\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import string\n",
    "\n",
    "\n",
    "def unroll(node, G):\n",
    "    if len(node.children) != 0:\n",
    "        for child in node.children:\n",
    "            G.add_edge(node.token['id'], child.token['id'])\n",
    "            unroll(child, G)\n",
    "    return G\n",
    "\n",
    "sentences = []\n",
    "data_file = open(\"UD_English-EWT/en_ewt-ud-train.conllu\", \"r\", encoding=\"utf-8\")\n",
    "for sentence in parse_incr(data_file):\n",
    "    sentences.append(sentence)\n",
    "    \n",
    "MIN_LENGTH = 10\n",
    "MAX_LENGTH = 50\n",
    "\n",
    "def check_length(sentence):\n",
    "    return len(sentence) < MAX_LENGTH and len(sentence) > MIN_LENGTH \n",
    "\n",
    "def filterSentences(sentences):\n",
    "    return [sent for sent in sentences if check_length(sent)]\n",
    "\n",
    "input_vocab = Vocab(\"ewt_train_trimmed\")\n",
    "filtered_sentences = filterSentences(sentences)\n",
    "\n",
    "sentences_text = []\n",
    "for sent in filtered_sentences:\n",
    "    input_vocab.addSentence(sent)\n",
    "    sentences_text.append(sent.metadata['text'])\n",
    "    \n",
    "dev_dict  = {}\n",
    "# max_node = 0\n",
    "for idx in range(0, len(filtered_sentences)):\n",
    "    curr_tree = filtered_sentences[idx].to_tree()\n",
    "    G_curr = nx.Graph()\n",
    "    G_curr = unroll(curr_tree, G_curr)\n",
    "    G = nx.relabel_nodes(G_curr, lambda x: x-1)\n",
    "    nx.write_edgelist(G, \"train/\"+str(idx)+\".edges\", data=False)\n",
    "    G_final = nx.convert_node_labels_to_integers(G_curr, ordering = \"decreasing degree\")\n",
    "#     if G_final.order()>max_node:\n",
    "#         max_node = G_final.order()\n",
    "#         max_idx = idx\n",
    "    nx.write_edgelist(G_final, \"ewt_train/\"+str(idx)+\".edges\", data=False)\n",
    "    dev_dict[idx] = list(G_final.edges)\n",
    "\n",
    "# print(max_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexesFromSentence(vocab, sentence):\n",
    "    return [vocab.word2index[token['form']] for token in sentence]\n",
    "\n",
    "def tensorFromSentence(vocab, sentence):\n",
    "    indexes = indexesFromSentence(vocab, sentence)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "def pairfromidx(idx):\n",
    "    input_tensor = tensorFromSentence(input_vocab, filtered_sentences[idx])\n",
    "    G = load_graph(\"train/\"+str(idx)+\".edges\")\n",
    "    target_matrix = get_dist_mat(G)\n",
    "    target_tensor = torch.from_numpy(target_matrix).float().to(device)\n",
    "    target_tensor.requires_grad = False\n",
    "    n = G.order()\n",
    "    return (input_tensor, target_tensor, n, sentences_text[idx])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)\n",
    "    \n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, max_length=MAX_LENGTH):\n",
    "        super(Attention, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.max_length = max_length\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        attention_scores = self.attn(torch.cat((embedded[0], hidden.unsqueeze(0)), 1))\n",
    "        attn_weights = F.softmax(attention_scores, dim=0)\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0), encoder_outputs.unsqueeze(0))\n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperbolic modules.\n",
    "\n",
    "class HypLinear(nn.Module):\n",
    "    \"\"\"Applies a hyperbolic \"linear\" transformation to the incoming data: :math:`y = xA^T + b`\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(HypLinear, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = nn.Parameter(torch.FloatTensor(out_features, in_features))\n",
    "\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.FloatTensor(1, out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, input_):\n",
    "        result = lu.torch_hyp_add(lu.torch_mv_mul_hyp(torch.transpose(self.weight,0,1), input_), self.bias) #(batch, input) x (input, output)\n",
    "        return result\n",
    "\n",
    "    def extra_repr(self):\n",
    "        return 'in_features={}, out_features={}, bias={}'.format(\n",
    "            self.in_features, self.out_features, self.bias is not None\n",
    "        )\n",
    "\n",
    "    \n",
    "class HyperbolicLSTMCell(nn.Module):\n",
    "\n",
    "    \"\"\"\n",
    "    Hyperbolic LSTM cell.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, use_bias=True):\n",
    "        super(HyperbolicLSTMCell, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.use_bias = use_bias\n",
    "        self.weight_ih = nn.Parameter(torch.FloatTensor(4*hidden_size, input_size))\n",
    "        self.weight_hh = nn.Parameter(torch.FloatTensor(4*hidden_size, hidden_size))\n",
    "        if use_bias:\n",
    "            self.bias = nn.Parameter(torch.FloatTensor(1, 4*hidden_size))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        \"\"\"\n",
    "        Initialize weight parameters.\n",
    "        Weight matrices -> Xavier uniform\n",
    "        bias -> constant 0\n",
    "        \"\"\"\n",
    "        torch.nn.init.xavier_uniform_(self.weight_ih.data)\n",
    "        torch.nn.init.xavier_uniform_(self.weight_hh.data)\n",
    "        #Set bias vectors to 0.\n",
    "        if self.use_bias:\n",
    "            torch.nn.init.constant_(self.bias.data, val=0)\n",
    "\n",
    "    def forward(self, input_, hx):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input: A (batch, input_size) tensor containing input\n",
    "                features.\n",
    "            hx: A tuple (h_0, c_0), which contains the initial hidden\n",
    "                and cell state, where the size of both states is\n",
    "                (batch_size, hidden_size).\n",
    "        Returns:\n",
    "            h_1, c_1: Tensors containing the next hidden and cell state.\n",
    "        \"\"\"\n",
    "        h_0, c_0 = hx\n",
    "\n",
    "        #torch_mv_mul_hyp does matmul(x, M) for Mx.\n",
    "        th = lu.torch_mv_mul_hyp(torch.transpose(self.weight_hh,0,1), h_0) #(batch, hidden) x (hidden, 4*hidden) = (batch, 4*hidden)\n",
    "        ti = lu.torch_mv_mul_hyp(torch.transpose(self.weight_ih,0,1), input_)  # (batch, input) x (input, 4*hidden) = (batch, 4*hidden)\n",
    "        rnn_out = lu.torch_hyp_add(lu.torch_hyp_add(th, ti), self.bias) #(batch, 4*hidden)\n",
    "\n",
    "        f, i, o, g = torch.split(rnn_out, split_size_or_sections= self.hidden_size, dim=1) #Each (batch, hidden)\n",
    "\n",
    "        f = lu.torch_exp_map_zero(torch.sigmoid(lu.torch_log_map_zero(f)))\n",
    "        i = lu.torch_exp_map_zero(torch.sigmoid(lu.torch_log_map_zero(i)))\n",
    "        o = lu.torch_exp_map_zero(torch.sigmoid(lu.torch_log_map_zero(o)))\n",
    "        g = lu.torch_exp_map_zero(torch.tanh(lu.torch_log_map_zero(g)))\n",
    "\n",
    "        c_1 = lu.torch_hyp_add(lu.torch_pointwise_prod(f, c_0), lu.torch_pointwise_prod(i, g))\n",
    "        c_1_nl = lu.torch_exp_map_zero(torch.tanh(lu.torch_log_map_zero(c_1)))\n",
    "        h_1 = lu.torch_pointwise_prod(o, c_1_nl)\n",
    "        \n",
    "        return h_1, c_1\n",
    "\n",
    "\n",
    "    def __repr__(self):\n",
    "        s = '{name}({input_size}, {hidden_size})'\n",
    "        return s.format(name=self.__class__.__name__, **self.__dict__)\n",
    "\n",
    "class HyperbolicLSTM(nn.Module):\n",
    "\n",
    "    \"\"\"A module that runs multiple steps of LSTM in Hyperbolic Space.\"\"\"\n",
    "    def __init__(self, cell_class, input_size, hidden_size, num_layers=1, use_bias=True, batch_first=False, **kwargs):\n",
    "        super(HyperbolicLSTM, self).__init__()\n",
    "        self.cell_class = cell_class\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.use_bias = use_bias\n",
    "        self.batch_first = batch_first\n",
    "\n",
    "        for layer in range(num_layers):\n",
    "            layer_input_size = input_size if layer == 0 else hidden_size\n",
    "            cell = cell_class(input_size=layer_input_size, hidden_size=hidden_size, **kwargs)\n",
    "            setattr(self, 'cell_{}'.format(layer), cell)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def get_cell(self, layer):\n",
    "        return getattr(self, 'cell_{}'.format(layer))\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for layer in range(self.num_layers):\n",
    "            cell = self.get_cell(layer)\n",
    "            cell.reset_parameters()\n",
    "\n",
    "    @staticmethod\n",
    "    def _forward_rnn(cell, input_, hx):\n",
    "        max_time = input_.size(0)\n",
    "        output = []\n",
    "        for time in range(max_time):\n",
    "            if isinstance(cell, HyperbolicLSTMCell):\n",
    "                h_next, c_next = cell(input_=input_[time], hx=hx)\n",
    "            else:\n",
    "                h_next, c_next = cell(input_=input_[time], hx=hx)\n",
    "            hx_next = (h_next, c_next)\n",
    "            output.append(h_next)\n",
    "            hx = hx_next\n",
    "        output = torch.stack(output, 0)\n",
    "        return output, hx\n",
    "\n",
    "    def forward(self, input_, hx=None):\n",
    "        if self.batch_first:\n",
    "            input_ = input_.transpose(0, 1)\n",
    "        max_time, batch_size, _ = input_.size()\n",
    "        if hx is None:\n",
    "            hx = (torch.zeros(self.num_layers, batch_size, self.hidden_size), torch.zeros(self.num_layers, batch_size, self.hidden_size))\n",
    "        h_n = []\n",
    "        c_n = []\n",
    "        layer_output = None\n",
    "        for layer in range(self.num_layers):\n",
    "            cell = self.get_cell(layer)\n",
    "            hx_layer = (hx[0][layer,:,:], hx[1][layer,:,:])\n",
    "        \n",
    "            if layer == 0:\n",
    "                layer_output, (layer_h_n, layer_c_n) = HyperbolicLSTM._forward_rnn(cell=cell, input_=input_, hx=hx_layer)\n",
    "            else:\n",
    "                layer_output, (layer_h_n, layer_c_n) = HyperbolicLSTM._forward_rnn(cell=cell, input_=layer_output, hx=hx_layer)\n",
    "        \n",
    "            h_n.append(layer_h_n)\n",
    "            c_n.append(layer_c_n)\n",
    "        output = layer_output\n",
    "        h_n = torch.stack(h_n, 0)\n",
    "        c_n = torch.stack(c_n, 0)\n",
    "        return output, (h_n, c_n)              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainWAttention(input_tensor, ground_truth, n, encoder, encoder_optimizer, attention, attention_optimizer, max_length=MAX_LENGTH):\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "    encoder_optimizer.zero_grad()\n",
    "    attention_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = ground_truth.size(0)\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "    encoder_hiddens = torch.zeros(input_length, encoder.hidden_size, device=device)\n",
    "    final_embeddings = torch.zeros(input_length, encoder.hidden_size, device=device)\n",
    "\n",
    "    loss = 0\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "        encoder_hiddens[ei] = encoder_hidden[0, 0]\n",
    "        \n",
    "    for idx in range(input_length):\n",
    "        output = attention(input_tensor[idx], encoder_hiddens[idx], encoder_outputs)\n",
    "        final_embeddings[idx] = output[0]\n",
    "        \n",
    "    loss += distortion(ground_truth, final_embeddings, n)\n",
    "    loss.backward()\n",
    "    encoder_optimizer.step()\n",
    "    attention_optimizer.step()\n",
    "\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(input_tensor, ground_truth, n, encoder, encoder_optimizer, max_length=MAX_LENGTH):\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "    encoder_optimizer.zero_grad()\n",
    "    encoder.train()\n",
    " \n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = ground_truth.size(0)\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "    loss = 0\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "    \n",
    "#     criterion = nn.L1Loss()\n",
    "#     ground_truth = torch.ones(encoder_outputs.shape, device=device)\n",
    "    loss += criterion(ground_truth, encoder_outputs)\n",
    "    loss.backward()\n",
    "    encoder_optimizer.step()\n",
    "\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(encoder, attention, n_iters=1000, print_every=100, plot_every=100, learning_rate=0.1):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  \n",
    "    plot_loss_total = 0  \n",
    "\n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    attention_optimizer = optim.SGD(attention.parameters(), lr=learning_rate)\n",
    "    training_pairs = [pairfromidx(idx) for idx in range(n_iters)]\n",
    "\n",
    "    for iter in range(1, n_iters + 1):     \n",
    "        training_pair = training_pairs[iter - 1]\n",
    "        input_tensor = training_pair[0]\n",
    "        target_matrix = training_pair[1]\n",
    "        n = training_pair[2]\n",
    "        loss = trainWAttention(input_tensor, target_matrix, n, encoder, encoder_optimizer, attention, attention_optimizer)\n",
    "#         loss = train(input_tensor, target_matrix, n, encoder, encoder_optimizer)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if iter % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
    "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
    "\n",
    "        if iter % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0m 39s (- 5m 56s) (100 10%) 0.6282\n",
      "1m 12s (- 4m 48s) (200 20%) 0.5995\n",
      "1m 43s (- 4m 2s) (300 30%) 0.5841\n",
      "2m 13s (- 3m 19s) (400 40%) 0.5704\n",
      "2m 44s (- 2m 44s) (500 50%) 0.5366\n",
      "3m 12s (- 2m 8s) (600 60%) 0.5354\n",
      "3m 41s (- 1m 34s) (700 70%) 0.5388\n",
      "4m 8s (- 1m 2s) (800 80%) 0.5519\n",
      "4m 38s (- 0m 30s) (900 90%) 0.5333\n",
      "5m 8s (- 0m 0s) (1000 100%) 0.5114\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 200\n",
    "encoder = EncoderLSTM(input_vocab.n_words, hidden_size).to(device)\n",
    "attention = Attention(input_vocab.n_words, hidden_size).to(device)\n",
    "trainIters(encoder, attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
