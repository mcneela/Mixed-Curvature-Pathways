{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse.csgraph as csg\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "import networkx as nx\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import math\n",
    "\n",
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distortion calculations\n",
    "\n",
    "def entry_is_good(h, h_rec): return (not torch.isnan(h_rec)) and (not torch.isinf(h_rec)) and h_rec != 0 and h != 0\n",
    "\n",
    "def distortion_entry(h,h_rec,me,mc):\n",
    "    avg = abs(h_rec - h)/h\n",
    "    if h_rec/h > me: me = h_rec/h\n",
    "    if h/h_rec > mc: mc = h/h_rec\n",
    "    return (avg,me,mc)\n",
    "\n",
    "def distortion_row(H1, H2, n, row):\n",
    "    mc, me, avg, good = 0,0,0,0\n",
    "    for i in range(n):\n",
    "        if i != row and entry_is_good(H1[i], H2[i]):\n",
    "            (_avg,me,mc) = distortion_entry(H1[i], H2[i],me,mc)\n",
    "            good        += 1\n",
    "            avg         += _avg\n",
    "    avg /= good if good > 0 else 1.0\n",
    "    return (mc, me, avg, n-1-good)\n",
    "\n",
    "def distortion(H1, H2, n, jobs=16):\n",
    "    dists = Parallel(n_jobs=jobs)(delayed(distortion_row)(H1[i,:],H2[i,:],n,i) for i in range(n))\n",
    "    avg = torch.stack([tup[2] for tup in dists]).sum()/n\n",
    "    return avg\n",
    "\n",
    "\n",
    "#Loading the graph and getting the distance matrix.\n",
    "\n",
    "def load_graph(file_name, directed=False):\n",
    "    G = nx.DiGraph() if directed else nx.Graph()\n",
    "    with open(file_name, \"r\") as f:\n",
    "        for line in f:\n",
    "            tokens = line.split()\n",
    "            u = int(tokens[0])\n",
    "            v = int(tokens[1])\n",
    "            if len(tokens) > 2:\n",
    "                w = float(tokens[2])\n",
    "                G.add_edge(u, v, weight=w)\n",
    "            else:\n",
    "                G.add_edge(u,v)\n",
    "    return G\n",
    "\n",
    "\n",
    "def compute_row(i, adj_mat): \n",
    "    return csg.dijkstra(adj_mat, indices=[i], unweighted=True, directed=False)\n",
    "\n",
    "def get_dist_mat(G):\n",
    "    n = G.order()\n",
    "    adj_mat = nx.to_scipy_sparse_matrix(G, nodelist=list(range(G.order())))\n",
    "    t = time.time()\n",
    "    \n",
    "    num_cores = multiprocessing.cpu_count()\n",
    "    dist_mat = Parallel(n_jobs=num_cores)(delayed(compute_row)(i,adj_mat) for i in range(n))\n",
    "    dist_mat = np.vstack(dist_mat)\n",
    "    return dist_mat\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n",
    "\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {}\n",
    "        self.n_words = 0\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for token in sentence:\n",
    "            self.addWord(token['form'])\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "            \n",
    "\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "from conllu import parse_tree, parse_tree_incr, parse, parse_incr\n",
    "from io import open\n",
    "import scipy.sparse.csgraph as csg\n",
    "import networkx as nx\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import string\n",
    "\n",
    "\n",
    "def unroll(node, G):\n",
    "    if len(node.children) != 0:\n",
    "        for child in node.children:\n",
    "            G.add_edge(node.token['id'], child.token['id'])\n",
    "            unroll(child, G)\n",
    "    return G\n",
    "\n",
    "sentences = []\n",
    "data_file = open(\"UD_English-EWT/en_ewt-ud-train.conllu\", \"r\", encoding=\"utf-8\")\n",
    "for sentence in parse_incr(data_file):\n",
    "    sentences.append(sentence)\n",
    "    \n",
    "MIN_LENGTH = 10\n",
    "MAX_LENGTH = 50\n",
    "\n",
    "def check_length(sentence):\n",
    "    return len(sentence.metadata['text'].split(' ')) < MAX_LENGTH and \\\n",
    "        len(sentence.metadata['text'].split(' ')) > MIN_LENGTH \n",
    "\n",
    "def filterSentences(sentences):\n",
    "    return [sent for sent in sentences if check_length(sent)]\n",
    "\n",
    "input_vocab = Vocab(\"ewt_train_trimmed\")\n",
    "filtered_sentences = filterSentences(sentences)\n",
    "\n",
    "sentences_text = []\n",
    "for sent in filtered_sentences:\n",
    "    input_vocab.addSentence(sent)\n",
    "    sentences_text.append(sent.metadata['text'])\n",
    "    \n",
    "dev_dict  = {}\n",
    "for idx in range(0, len(filtered_sentences)):\n",
    "    curr_tree = filtered_sentences[idx].to_tree()\n",
    "    G_curr = nx.Graph()\n",
    "    G_curr = unroll(curr_tree, G_curr)\n",
    "    G = nx.relabel_nodes(G_curr, lambda x: x-1)\n",
    "    nx.write_edgelist(G, \"train/\"+str(idx)+\".edges\", data=False)\n",
    "    G_final = nx.convert_node_labels_to_integers(G_curr, ordering = \"decreasing degree\")\n",
    "    nx.write_edgelist(G_final, \"train_reordered/\"+str(idx)+\".edges\", data=False)\n",
    "    dev_dict[idx] = list(G_final.edges)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexesFromSentence(vocab, sentence):\n",
    "    return [vocab.word2index[token['form']] for token in sentence]\n",
    "\n",
    "def tensorFromSentence(vocab, sentence):\n",
    "    indexes = indexesFromSentence(vocab, sentence)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "def pairfromidx(idx):\n",
    "    input_tensor = tensorFromSentence(input_vocab, filtered_sentences[idx])\n",
    "    G = load_graph(\"train/\"+str(idx)+\".edges\")\n",
    "    target_matrix = get_dist_mat(G)\n",
    "    target_tensor = torch.from_numpy(target_matrix).float().to(device)\n",
    "    n = G.order()\n",
    "    return (input_tensor, target_tensor, n, sentences_text[idx])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(input_tensor, ground_truth, n, encoder, encoder_optimizer, max_length=MAX_LENGTH):\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "    encoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = ground_truth.size(0)\n",
    "    encoder_outputs = torch.zeros(input_length, encoder.hidden_size, device=device)\n",
    "\n",
    "    loss = 0\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "        \n",
    "    loss += distortion(encoder_outputs, ground_truth, n)\n",
    "    loss.backward()\n",
    "    encoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / input_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(encoder, n_iters=2000, print_every=500, plot_every=100, learning_rate=1.0):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  \n",
    "    plot_loss_total = 0  \n",
    "\n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    training_pairs = [pairfromidx(idx) for idx in range(n_iters)]\n",
    "\n",
    "    for iter in range(1, n_iters + 1):\n",
    "\n",
    "        training_pair = training_pairs[iter - 1]\n",
    "        input_tensor = training_pair[0]\n",
    "        target_matrix = training_pair[1]\n",
    "        n = training_pair[2]\n",
    "        \n",
    "        loss = train(input_tensor, target_matrix, n, encoder, encoder_optimizer)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if iter % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
    "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
    "\n",
    "        if iter % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "\n",
    "    showPlot(plot_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 100\n",
    "encoder = EncoderLSTM(input_vocab.n_words, hidden_size).to(device)\n",
    "trainIters(encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
