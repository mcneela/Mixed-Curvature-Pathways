universe = vanilla

# the script that will be run when the job starts
executable = run.sh

# include the cluster id and process id that are set at runtime
log = output/condor_logs/job_$(Cluster)_$(Process).log
error = output/condor_logs/job_$(Cluster)_$(Process).err
output = output/condor_logs/job_$(Cluster)_$(Process).out

# these files get transferred from the submit node to the server on which the program is executing
# transfer over code, data, and arguments
transfer_input_files = args.tsv, wrapper.py, run.sh, data.tar.gz, code.tar.gz, env.tar.gz
should_transfer_files = YES
when_to_transfer_output = ON_EXIT
transfer_output_files = output
preserve_relative_paths = true

# checkpointing setup (it's ok if not using checkpointing, can still leave these lines here)
#checkpoint_exit_code = 85
#transfer_checkpoint_files = output/training_logs/

request_cpus = 4
request_memory = 16GB
request_disk = 20GB

+WantFlocking = true

# these environment variables will be set on the execute node
# useful for printing info and setting GitHub, WandB keys
environment = "CLUSTER=$(Cluster) PROCESS=$(Process) RUNNINGON=$$(Name) GITHUB_TAG=$ENV(GITHUB_TAG) WANDB_API_KEY=$ENV(WANDB_API_KEY)"

queue 5
