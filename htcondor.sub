universe = vanilla

# the script that will be run when the job starts
executable = run.sh

# include the cluster id and process id that are set at runtime
log = output/condor_logs/job_$(Cluster)_$(cProcess).log
error = output/condor_logs/job_$(Cluster)_$(cProcess).err
output = output/condor_logs/job_$(Cluster)_$(cProcess).out

# these files get transferred from the submit node to the server on which the program is executing
# transfer over code, data, and arguments
transfer_input_files = run.sh, code.tar.gz, args/$(cProcess).txt, output/training_logs/$(UUID), {transfer_input_files}
should_transfer_files = YES
when_to_transfer_output = ON_EXIT
transfer_output_files = output/training_logs/$(UUID)
preserve_relative_paths = true

# checkpointing setup (it's ok if not using checkpointing, can still leave these lines here)
checkpoint_exit_code = 85
transfer_checkpoint_files = output/training_logs/$(UUID)

requirements = (CUDADriverVersion >= 11.2) && (CUDACapability > 3.5)
request_gpus = 1

+WantGPULab = true
+GPUJobLength = "short"

request_cpus = 4
request_memory = 16GB
request_disk = 20GB

+WantFlocking = true

# these environment variables will be set on the execute node
# useful for printing info and setting GitHub, WandB keys
environment = "CLUSTER=$(Cluster) PROCESS=$(cProcess) RUNNINGON=$$(Name) GITHUB_TAG=$ENV(GITHUB_TAG) WANDB_API_KEY=$ENV(WANDB_API_KEY)"

# this is how many jobs to queue
queue cProcess,UUID from queue.txt
